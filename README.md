# ğŸš€ Kabum\! Web Scraper de AnÃºncios de Celulares

[](https://www.google.com/search?q=https://github.com/SEU_USUARIO/SEU_REPOSITORIO)
[](https://opensource.org/licenses/MIT)
[](https://www.python.org/downloads/)

Um projeto de web scraping robusto e containerizado, desenvolvido para extrair, tratar e armazenar dados de anÃºncios de celulares do e-commerce KaBuM\!. A soluÃ§Ã£o completa utiliza Selenium e BeautifulSoup4 para a coleta de dados, salva os resultados brutos em um arquivo CSV, utiliza um Jupyter Notebook com Pandas e NumPy para o processo de ETL, e por fim, carrega os dados tratados em um banco de dados PostgreSQL. Todo o ambiente Ã© orquestrado com Docker, garantindo simplicidade na configuraÃ§Ã£o e execuÃ§Ã£o.

## ğŸ“‹ Ãndice

  - [VisÃ£o Geral do Projeto]
  - [âœ¨ Funcionalidades]
  - [ğŸ› ï¸ Tecnologias Utilizadas]
  - [ğŸ”§ PrÃ©-requisitos]
  - [âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o]
  - [ğŸ“ Estrutura do Projeto]
  - [ğŸ—ƒï¸ Schema do Banco de Dados]


## ğŸ“– VisÃ£o Geral do Projeto

Este projeto automatiza a coleta de dados de celulares no site da KaBuM\!. O fluxo Ã© dividido em duas etapas principais:

1.  **ExtraÃ§Ã£o (`Extract.py`):** Um script Python que navega por aproximadamente 500 pÃ¡ginas do site, coleta os dados brutos dos anÃºncios e os salva no arquivo `produtos.csv`.
2.  **TransformaÃ§Ã£o e Carga (`Transform Code.ipynb`):** Um Jupyter Notebook que lÃª os dados brutos do CSV, realiza todo o processo de limpeza, transformaÃ§Ã£o e enriquecimento dos dados (ETL), e por fim, carrega os dados limpos na tabela do banco de dados PostgreSQL.

## âœ¨ Funcionalidades

  - **Scraping DinÃ¢mico:** Utiliza **Selenium** para navegar por mÃºltiplas pÃ¡ginas, lidando com JavaScript e carregamento dinÃ¢mico.
  - **ExtraÃ§Ã£o HTML Eficiente:** Emprega **BeautifulSoup4 (bs4)** para fazer o parse do HTML e extrair os dados.
  - **ETL com Jupyter Notebook:** Realiza a limpeza e transformaÃ§Ã£o dos dados de forma interativa e documentada usando **Pandas** e **NumPy**.
  - **Fluxo de Dados Persistido:** Salva os dados brutos (`produtos.csv`) e tratados (`WebScraping-Kabum-Tradados.csv`), permitindo reanÃ¡lise sem a necessidade de um novo scraping.
  - **Armazenamento Robusto:** Salva os dados finais em um banco de dados **PostgreSQL**.
  - **Ambiente Containerizado:** Todo o projeto Ã© executado em contÃªineres **Docker**, garantindo um ambiente consistente e isolado.

## ğŸ› ï¸ Tecnologias Utilizadas

  - **Linguagem:** Python 3.9+
  - **Web Scraping:** `Selenium`, `BeautifulSoup4`
  - **ETL e AnÃ¡lise de Dados:** `Pandas`, `NumPy`, `Jupyter Notebook`
  - **Banco de Dados:** `PostgreSQL`, `Psycopg2-binary`
  - **ContainerizaÃ§Ã£o:** `Docker`, `Docker Compose`

## ğŸ”§ PrÃ©-requisitos

Para executar este projeto localmente, vocÃª precisarÃ¡ ter as seguintes ferramentas instaladas:

  - [Docker](https://www.docker.com/get-started)
  - [Docker Compose](https://docs.docker.com/compose/install/)

## âš™ï¸ InstalaÃ§Ã£o e ExecuÃ§Ã£o

O processo Ã© dividido em trÃªs etapas: **configurar o ambiente**, **executar a extraÃ§Ã£o** e **executar a transformaÃ§Ã£o/carga**.

### 1\. ConfiguraÃ§Ã£o do Ambiente

1.  **Clone o repositÃ³rio:**

    ```bash
    git clone https://github.com/Marcelo2506/WebScraping-Kabum.git
    cd WebScraping-Kabum
    ```

2.  **Crie e configure o arquivo `.env`:**
    Copie o arquivo de exemplo e preencha com suas credenciais do banco de dados.

    ```bash
    cp .env.example .env
    ```

3.  **Construa e inicie os contÃªineres:**
    Este comando irÃ¡ iniciar o serviÃ§o do banco de dados e o serviÃ§o da aplicaÃ§Ã£o (`scraper`).

    ```bash
    docker-compose up --build -d
    ```

### 2\. Executar a ExtraÃ§Ã£o

Execute o script `Extract.py` dentro do contÃªiner para iniciar o web scraping. Ele irÃ¡ gerar o arquivo `produtos.csv`.

```bash
docker-compose exec scraper python Extract.py
```

### 3\. Executar a TransformaÃ§Ã£o e Carga

Execute o Jupyter Notebook para tratar os dados do `produtos.csv` e carregÃ¡-los no PostgreSQL.

```bash
docker-compose exec scraper jupyter nbconvert --to notebook --execute "Transform Code.ipynb"
```

Este comando executa todas as cÃ©lulas do notebook do inÃ­cio ao fim. Ao final, os dados estarÃ£o no banco de dados e o arquivo `WebScraping-Kabum-Tradados.csv` serÃ¡ gerado.

### Finalizando

  - **Para verificar os dados no banco:** Conecte-se ao PostgreSQL (host `localhost`, porta `5432`) usando as credenciais do seu arquivo `.env`.
  - **Para parar todos os serviÃ§os:**
    ```bash
    docker-compose down
    ```

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Extract.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ produtos.csv
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Transform Code.ipynb
â””â”€â”€ WebScraping-Kabum-Tradados.csv
```

## ğŸ—ƒï¸ Schema do Banco de Dados

Os dados tratados sÃ£o armazenados em uma tabela chamada `celulares` (ou similar) no banco de dados `kabum_db`.

| Coluna | Tipo de Dado | DescriÃ§Ã£o |
| :--- | :--- | :--- |
| `id` | `SERIAL PRIMARY KEY`| Identificador Ãºnico do registro. |
| `nome_produto` | `VARCHAR(255)` | Nome completo do celular anunciado. |
| `preco_atual` | `DECIMAL(10, 2)`| PreÃ§o principal do produto. |
| ... | ... | (demais colunas conforme definidas no notebook) |
| `data_coleta` | `TIMESTAMP` | Data e hora em que o dado foi coletado. |

